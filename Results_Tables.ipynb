{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c793880d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/melanie/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0885afcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewsAgendas with agenda scores (461, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load NewsAgendas, skip articles without an annotated agenda score\n",
    "full_data = pd.read_json('newsagendas.jsonl', lines=True)\n",
    "full_data = full_data[full_data['annotated-agenda-score'] != 'no answer']\n",
    "keep_indices = list(full_data.index)\n",
    "print('NewsAgendas with agenda scores', full_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caefc03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article-title</th>\n",
       "      <th>article-contents</th>\n",
       "      <th>annotated-labels</th>\n",
       "      <th>annotated-agenda-score</th>\n",
       "      <th>annotated-evidence</th>\n",
       "      <th>split</th>\n",
       "      <th>weak-label-0</th>\n",
       "      <th>weak-label-1</th>\n",
       "      <th>weak-label-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>More women than ever are freezing their eggs w...</td>\n",
       "      <td>NewsBioethics&lt;/br&gt;&lt;/br&gt;LEICESTER, England, Sep...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clickbait': ['There are a variety of reasons...</td>\n",
       "      <td>test</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>bias</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nikki Haley Blasts UN for Anti-Israel Vote - S...</td>\n",
       "      <td>U.S. Ambassador to the United Nations Nikki Ha...</td>\n",
       "      <td>politicalbias clickbait propaganda</td>\n",
       "      <td>4</td>\n",
       "      <td>{'clickbait': ['This is not the Muslim apologi...</td>\n",
       "      <td>test</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>bias</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Why JPMorgan Believes Central Banks Can No Lon...</td>\n",
       "      <td>In recent weeks, JPMorgan has turned decidedly...</td>\n",
       "      <td>conspiracytheory</td>\n",
       "      <td>3</td>\n",
       "      <td>{'conspiracytheory': ['But, to us, the fundame...</td>\n",
       "      <td>test</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Celebrities To Leave US After Election of Raci...</td>\n",
       "      <td>By Ivan Fernando&lt;/br&gt;&lt;/br&gt;On November 8th ever...</td>\n",
       "      <td>politicalbias</td>\n",
       "      <td>2</td>\n",
       "      <td>{'politicalbias': ['On November 8th everything...</td>\n",
       "      <td>test</td>\n",
       "      <td>satire</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Fmr. Congressman and Korean War Vet Calls for ...</td>\n",
       "      <td>\"As a partially disabled army veteran myself, ...</td>\n",
       "      <td>politicalbias</td>\n",
       "      <td>2</td>\n",
       "      <td>{'politicalbias': ['Shamansky took on the Bush...</td>\n",
       "      <td>test</td>\n",
       "      <td>political</td>\n",
       "      <td>clickbait</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      article-title  \\\n",
       "0   0  More women than ever are freezing their eggs w...   \n",
       "2   2  Nikki Haley Blasts UN for Anti-Israel Vote - S...   \n",
       "3   3  Why JPMorgan Believes Central Banks Can No Lon...   \n",
       "4   4  Celebrities To Leave US After Election of Raci...   \n",
       "5   5  Fmr. Congressman and Korean War Vet Calls for ...   \n",
       "\n",
       "                                    article-contents  \\\n",
       "0  NewsBioethics</br></br>LEICESTER, England, Sep...   \n",
       "2  U.S. Ambassador to the United Nations Nikki Ha...   \n",
       "3  In recent weeks, JPMorgan has turned decidedly...   \n",
       "4  By Ivan Fernando</br></br>On November 8th ever...   \n",
       "5  \"As a partially disabled army veteran myself, ...   \n",
       "\n",
       "                     annotated-labels annotated-agenda-score  \\\n",
       "0                           clickbait                      2   \n",
       "2  politicalbias clickbait propaganda                      4   \n",
       "3                    conspiracytheory                      3   \n",
       "4                       politicalbias                      2   \n",
       "5                       politicalbias                      2   \n",
       "\n",
       "                                  annotated-evidence split weak-label-0  \\\n",
       "0  {'clickbait': ['There are a variety of reasons...  test    clickbait   \n",
       "2  {'clickbait': ['This is not the Muslim apologi...  test   propaganda   \n",
       "3  {'conspiracytheory': ['But, to us, the fundame...  test   conspiracy   \n",
       "4  {'politicalbias': ['On November 8th everything...  test       satire   \n",
       "5  {'politicalbias': ['Shamansky took on the Bush...  test    political   \n",
       "\n",
       "  weak-label-1 weak-label-2  \n",
       "0         bias               \n",
       "2         bias               \n",
       "3                            \n",
       "4                            \n",
       "5    clickbait               "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa7aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather maliciousness scores\n",
    "full_labels = list(full_data['annotated-agenda-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f38b5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sentiment scores\n",
    "def gen_sentiment_labels(df):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df['text'] = df['article-title'] + ' ' + df['article-contents']\n",
    "    pred_scores, true_scores = [], []\n",
    "    for row in df.iterrows():\n",
    "        if str(row[1]['annotated-labels']) == 'nan':\n",
    "            tags = ''\n",
    "        else:\n",
    "            tags = row[1]['annotated-labels'].strip().split(' ')\n",
    "        score = sia.polarity_scores(row[1]['text'])\n",
    "        pred_scores.append(int(score['compound'] < 0))\n",
    "        true_scores.append(int('negativesentiment' in tags))\n",
    "    return pred_scores, true_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d206ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather weak labels\n",
    "def get_weak_labels(df):\n",
    "    weak_label0s = list(df['weak-label-0'])\n",
    "    weak_label1s = list(df['weak-label-1'])\n",
    "    weak_label2s = list(df['weak-label-2'])\n",
    "    weak_labels_mess = np.array([weak_label0s, weak_label1s, weak_label2s]).transpose()\n",
    "    weak_labels = []\n",
    "    for x in weak_labels_mess:\n",
    "        row = []\n",
    "        for y in x:\n",
    "            y = y.strip()\n",
    "            if y and y != 'nan':\n",
    "                row.append(y)\n",
    "        weak_labels.append(row)\n",
    "    return weak_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83d532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather annotated labels\n",
    "def get_annot_labels(df):\n",
    "    annot_labels = list(df['annotated-labels'])\n",
    "    annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n",
    "    annot_labels = list(annot_labels)\n",
    "    return annot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bc86a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of interest\n",
    "features = ['hatespeech', 'junkscience', 'propaganda', 'satire', 'clickbait', 'conspiracytheory']\n",
    "weak_label_key = {\n",
    "    \"hatespeech\":\"hate\",\n",
    "    \"junkscience\":\"junksci\",\n",
    "    \"propaganda\":\"propaganda\",\n",
    "    \"satire\":\"satire\",\n",
    "    \"clickbait\":\"clickbait\",\n",
    "    \"conspiracytheory\":\"conspiracy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98faf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a model predictions file, align index\n",
    "def load_pred_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = [eval(x.strip()) for x in f.readlines()]\n",
    "    full_preds = np.array([int(x['predicted_label']) for x in data if x['annotation_id'].startswith('old')])\n",
    "    full_preds = list(full_preds[keep_indices])\n",
    "    return full_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe0d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate scores for model predictions\n",
    "def score_preds(true, preds):\n",
    "    precision0 = metrics.precision_score(true, preds, pos_label=0, average='binary')\n",
    "    precision1 = metrics.precision_score(true, preds, pos_label=1, average='binary')\n",
    "    recall0 = metrics.recall_score(true, preds, pos_label=0, average='binary')\n",
    "    recall1 = metrics.recall_score(true, preds, pos_label=1, average='binary')\n",
    "    accuracy = metrics.accuracy_score(true, preds)\n",
    "    balacc = metrics.accuracy_score(true, preds)\n",
    "    f1_0 = metrics.f1_score(true, preds, pos_label=0, average='binary')\n",
    "    f1_1 = metrics.f1_score(true, preds, pos_label=1, average='binary')\n",
    "    iou = metrics.jaccard_score(true, preds)\n",
    "    f1macro = metrics.f1_score(true, preds, average='macro')\n",
    "    results = {\n",
    "        'Precision':[round(precision0*100, 1), round(precision1*100, 1)],\n",
    "        'Recall':[round(recall0*100, 1), round(recall1*100, 1)],\n",
    "        'F1': [round(f1_0*100, 1), round(f1_1*100, 1)],\n",
    "        'Accuracy':[round(accuracy*100, 1), -1],\n",
    "        'Balanced Accuracy': [round(balacc*100, 1), -1],\n",
    "        'F1-Macro':[round(f1macro*100, 1), -1],\n",
    "        'iou':[round(iou*100, 1), -1],\n",
    "    }\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a50c7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- hatespeech -----\n",
      "% labeled 1: 15.4\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       84.6    88.7  86.6      76.8               76.8      49.8  7.0\n",
      "1       15.4    11.3  13.0      -1.0               -1.0      -1.0 -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       83.7    87.9  85.8      75.3               75.3      46.2  3.4\n",
      "1        7.8     5.6   6.6      -1.0               -1.0      -1.0 -1.0\n",
      "\n",
      "----- junkscience -----\n",
      "% labeled 1: 4.3\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       95.4    94.1  94.7      90.0               90.0      47.4  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       95.5    85.9  90.5      82.6               82.6      47.6  2.4\n",
      "1        3.1    10.0   4.8      -1.0               -1.0      -1.0 -1.0\n",
      "\n",
      "----- propaganda -----\n",
      "% labeled 1: 42.1\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       54.0    70.0  61.0      48.2               48.2      41.8  12.8\n",
      "1       30.4    18.0  22.7      -1.0               -1.0      -1.0  -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       56.2    67.8  61.5      50.8               50.8      46.6  18.9\n",
      "1       38.1    27.3  31.8      -1.0               -1.0      -1.0  -1.0\n",
      "\n",
      "----- satire -----\n",
      "% labeled 1: 11.1\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       88.9   100.0  94.1      88.9               88.9      47.1  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       88.9    99.8  94.0      88.7               88.7      47.0  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "\n",
      "----- clickbait -----\n",
      "% labeled 1: 23.9\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       75.5    80.9  78.1      65.5               65.5      48.3  10.2\n",
      "1       21.2    16.4  18.5      -1.0               -1.0      -1.0  -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       76.5    80.6  78.5      66.4               66.4      50.7  12.9\n",
      "1       25.3    20.9  22.9      -1.0               -1.0      -1.0  -1.0\n",
      "\n",
      "----- conspiracytheory -----\n",
      "% labeled 1: 14.3\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       84.5    58.0  68.8      54.9               54.9      43.8  10.3\n",
      "1       12.6    36.4  18.8      -1.0               -1.0      -1.0  -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       84.7    50.4  63.2      49.7               49.7      41.9  11.5\n",
      "1       13.3    45.5  20.5      -1.0               -1.0      -1.0  -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/factcheck/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Model prediction results against weak labels\n",
    "for feat in features:\n",
    "    print(f'\\n----- {feat} -----')\n",
    "    bert_path = os.path.join('results', f'{feat}_bert_predictions.json')\n",
    "    fresh_path = os.path.join('results', f'{feat}_fresh_predictions.json') \n",
    "    bert_full_labels = load_pred_file(bert_path)\n",
    "    fresh_full_labels = load_pred_file(fresh_path)\n",
    "    weakfeat = weak_label_key[feat]\n",
    "    true_labels = [weakfeat in x for x in get_weak_labels(full_data)]\n",
    "    print(f'% labeled 1: {round(sum(true_labels)*100/len(true_labels),1)}\\n')\n",
    "    print('BERT')\n",
    "    bert_results = score_preds(true_labels, bert_full_labels)\n",
    "    print(bert_results)\n",
    "    print('FRESH')\n",
    "    fresh_results = score_preds(true_labels, fresh_full_labels)\n",
    "    print(fresh_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd29600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- hatespeech -----\n",
      "% labeled 1: 10.6\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       89.0    88.3  88.7      79.8               79.8      48.3  4.1\n",
      "1        7.7     8.2   7.9      -1.0               -1.0      -1.0 -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       88.5    88.1  88.3      79.2               79.2      46.2  2.0\n",
      "1        3.9     4.1   4.0      -1.0               -1.0      -1.0 -1.0\n",
      "WEAK\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       89.4   100.0  94.4      89.4               89.4      47.2  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "\n",
      "----- junkscience -----\n",
      "% labeled 1: 2.6\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       97.2    94.2  95.7      91.8               91.8      47.9  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       97.2    86.0  91.3      83.9               83.9      46.9  1.3\n",
      "1        1.6     8.3   2.6      -1.0               -1.0      -1.0 -1.0\n",
      "WEAK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n",
      "/usr/local/Caskroom/miniconda/base/envs/factcheck/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n",
      "/usr/local/Caskroom/miniconda/base/envs/factcheck/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       97.4   100.0  98.7      97.4               97.4      49.3  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "\n",
      "----- propaganda -----\n",
      "% labeled 1: 36.9\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       60.7    72.2  65.9      52.9               52.9      44.9  13.5\n",
      "1       29.6    20.0  23.9      -1.0               -1.0      -1.0  -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       61.2    67.7  64.3      52.5               52.5      46.7  17.0\n",
      "1       32.4    26.5  29.1      -1.0               -1.0      -1.0  -1.0\n",
      "WEAK\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       85.4    78.4  81.7      77.9               77.9      76.8  56.2\n",
      "1       67.5    77.1  72.0      -1.0               -1.0      -1.0  -1.0\n",
      "\n",
      "----- satire -----\n",
      "% labeled 1: 12.4\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       87.6   100.0  93.4      87.6               87.6      46.7  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       87.6    99.8  93.3      87.4               87.4      46.6  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n",
      "WEAK\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       94.6    96.0  95.3      91.8               91.8      80.1  47.9\n",
      "1       68.6    61.4  64.8      -1.0               -1.0      -1.0  -1.0\n",
      "\n",
      "----- clickbait -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n",
      "/usr/local/Caskroom/miniconda/base/envs/factcheck/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n",
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% labeled 1: 20.0\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       80.1    81.6  80.8      69.0               69.0      50.0  10.6\n",
      "1       20.0    18.5  19.2      -1.0               -1.0      -1.0  -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       80.0    80.2  80.1      68.1               68.1      49.9  10.9\n",
      "1       19.8    19.6  19.7      -1.0               -1.0      -1.0  -1.0\n",
      "WEAK\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       87.7    83.5  85.6      77.4               77.4      67.0  32.0\n",
      "1       44.5    53.3  48.5      -1.0               -1.0      -1.0  -1.0\n",
      "\n",
      "----- conspiracytheory -----\n",
      "% labeled 1: 16.7\n",
      "\n",
      "BERT\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       82.7    58.3  68.4      55.1               55.1      45.4  12.7\n",
      "1       15.8    39.0  22.5      -1.0               -1.0      -1.0  -1.0\n",
      "FRESH\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro   iou\n",
      "0       82.6    50.5  62.7      49.9               49.9      43.2  13.5\n",
      "1       15.9    46.8  23.8      -1.0               -1.0      -1.0  -1.0\n",
      "WEAK\n",
      "   Precision  Recall    F1  Accuracy  Balanced Accuracy  F1-Macro  iou\n",
      "0       83.3   100.0  90.9      83.3               83.3      45.4  0.0\n",
      "1        0.0     0.0   0.0      -1.0               -1.0      -1.0 -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/factcheck/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment recall 0.7352941176470589\n",
      "Sentiment recall 0.2403846153846154\n"
     ]
    }
   ],
   "source": [
    "#Model prediction results against annotated labels\n",
    "for feat in features:\n",
    "    print(f'\\n----- {feat} -----')\n",
    "    true_labels = [feat in x for x in get_annot_labels(full_data)]\n",
    "    bert_path = os.path.join('results', f'{feat}_bert_predictions.json')\n",
    "    fresh_path = os.path.join('results', f'{feat}_fresh_predictions.json') \n",
    "    bert_full_labels = load_pred_file(bert_path)\n",
    "    fresh_full_labels = load_pred_file(fresh_path)\n",
    "    print(f'% labeled 1: {round(sum(true_labels)*100/len(true_labels),1)}\\n')\n",
    "    print('BERT')\n",
    "    bert_results = score_preds(true_labels, bert_full_labels)\n",
    "    print(bert_results)\n",
    "    print('FRESH')\n",
    "    fresh_results = score_preds(true_labels, fresh_full_labels)\n",
    "    print(fresh_results)\n",
    "    print('WEAK')\n",
    "    weak_labels = get_weak_labels(full_data)\n",
    "    weak_labels = [int(feat in x) for x in weak_labels]\n",
    "    weak_results  = score_preds(true_labels, weak_labels)\n",
    "    print(weak_results)\n",
    "    \n",
    "sen_labels, true_sen_labels = gen_sentiment_labels(full_data)\n",
    "print(\"Sentiment recall\", metrics.recall_score(true_sen_labels, sen_labels, pos_label=1, average='binary'))\n",
    "print(\"Sentiment recall\", metrics.jaccard_score(true_sen_labels, sen_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5adf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xl/r4znp4jn61qfq11n2gpwm2lr0000gp/T/ipykernel_32338/1087762494.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  annot_labels = np.array([x.strip().split(' ') if not str(x) == 'nan' else '' for x in annot_labels]).transpose()\n"
     ]
    }
   ],
   "source": [
    "# Gather all of the predicted and labeled features\n",
    "full_vectors = {'BERT':[], 'FRESH':[], 'WEAK':[], 'ANNOT':[]}\n",
    "annot_full_labels = get_annot_labels(full_data)\n",
    "weak_full_labels = get_weak_labels(full_data)\n",
    "for feat in features:\n",
    "    bert_path = os.path.join('results', f'{feat}_bert_predictions.json')\n",
    "    fresh_path = os.path.join('results', f'{feat}_fresh_predictions.json')  \n",
    "    bert_full_labels = load_pred_file(bert_path)\n",
    "    fresh_full_labels = load_pred_file(fresh_path)\n",
    "    weak_labels = [int(feat in x) for x in weak_full_labels]\n",
    "    annot_labels = [int(feat in x) for x in annot_full_labels]\n",
    "    models_full_labels = {\n",
    "        'BERT':bert_full_labels,\n",
    "        'FRESH':fresh_full_labels,\n",
    "        'WEAK':weak_labels,\n",
    "        'ANNOT':annot_labels\n",
    "    }\n",
    "    for key in ['BERT', 'FRESH', 'WEAK', 'ANNOT']:\n",
    "        for i, lab in enumerate(models_full_labels[key]):\n",
    "            if i >= len(full_vectors[key]):\n",
    "                full_vectors[key].append([lab])\n",
    "            else:\n",
    "                full_vectors[key][i].append(lab)\n",
    "\n",
    "full_pred_sent, full_true_sent = gen_sentiment_labels(full_data)\n",
    "for key in ['BERT', 'FRESH']:\n",
    "    for i, lab in enumerate(full_pred_sent):\n",
    "        full_vectors[key][i].append(lab)\n",
    "\n",
    "for i, lab in enumerate(full_true_sent):\n",
    "    full_vectors['ANNOT'][i].append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e214f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 1 labels 41.6\n",
      "----- FRESH ------\n",
      "LogisticReg F1-0 0.6757004024107971\n",
      "LogisticReg F1-1 0.2592760230528993\n",
      "LogisticReg F1macro 0.4674882127318482\n",
      "LogisticReg accuracy 0.5565217391304349\n",
      "LogisticReg balanced accuracy 0.5194941262716093\n",
      "----- BERT ------\n",
      "LogisticReg F1-0 0.6846900923065399\n",
      "LogisticReg F1-1 0.28677158283420146\n",
      "LogisticReg F1macro 0.4857308375703706\n",
      "LogisticReg accuracy 0.5652173913043478\n",
      "LogisticReg balanced accuracy 0.5204200686035501\n",
      "----- WEAK ------\n",
      "LogisticReg F1-0 0.653448734695355\n",
      "LogisticReg F1-1 0.449193167388888\n",
      "LogisticReg F1macro 0.5513209510421215\n",
      "LogisticReg accuracy 0.5847826086956521\n",
      "LogisticReg balanced accuracy 0.576361935590478\n",
      "----- ANNOT ------\n",
      "LogisticReg F1-0 0.8065622540874153\n",
      "LogisticReg F1-1 0.6789804434382675\n",
      "LogisticReg F1macro 0.7427713487628413\n",
      "LogisticReg accuracy 0.7608695652173914\n",
      "LogisticReg balanced accuracy 0.7440875755076132\n"
     ]
    }
   ],
   "source": [
    "# Fit agenda clasification models with cross-validation\n",
    "\n",
    "bin_labels = [round((s-1)/5) for s in full_labels]\n",
    "print('% 1 labels', round(sum(bin_labels)/len(bin_labels)*100, 1))\n",
    "full_labs = [round((s-1)/5) for s in full_labels]\n",
    "for model in ['FRESH', 'BERT', 'WEAK', 'ANNOT']:\n",
    "    print(f'----- {model} ------')\n",
    "    lmodel = sklearn.linear_model.LogisticRegression()\n",
    "    full_vecs = full_vectors[model]\n",
    "    sz = int(0.1*len(full_vecs))\n",
    "    f10, f11, f1mac, acc, balacc = [], [], [], [], []\n",
    "    for i in range(10):\n",
    "        lmodel.fit(full_vecs[0:i*sz]+full_vecs[(i+1)*sz:], full_labs[0:i*sz]+full_labs[(i+1)*sz:])\n",
    "        preds = lmodel.predict(full_vecs[i*sz:(i+1)*sz])\n",
    "        f10.append(metrics.f1_score(full_labs[i*sz:(i+1)*sz], preds, pos_label=0))\n",
    "        f11.append(metrics.f1_score(full_labs[i*sz:(i+1)*sz], preds))\n",
    "        f1mac.append(metrics.f1_score(full_labs[i*sz:(i+1)*sz], preds, average='macro'))\n",
    "        acc.append(metrics.accuracy_score(full_labs[i*sz:(i+1)*sz], preds))    \n",
    "        balacc.append(metrics.balanced_accuracy_score(full_labs[i*sz:(i+1)*sz], preds))\n",
    "    print(\"LogisticReg F1-0\",np.mean(f10))\n",
    "    print(\"LogisticReg F1-1\",np.mean(f11))\n",
    "    print(\"LogisticReg F1macro\",np.mean(f1mac))\n",
    "    print(\"LogisticReg accuracy\",np.mean(acc))\n",
    "    print(\"LogisticReg balanced accuracy\", np.mean(balacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "868edf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----hate-----\n",
      "Balanced accuracy 0.7053730775669995\n",
      "-----junksci-----\n",
      "Balanced accuracy 0.7440875755076132\n",
      "-----prop-----\n",
      "Balanced accuracy 0.7252297815027708\n",
      "-----sat-----\n",
      "Balanced accuracy 0.7328845906904979\n",
      "-----click-----\n",
      "Balanced accuracy 0.7271380847322984\n",
      "-----conspiracy-----\n",
      "Balanced accuracy 0.7359709183756529\n",
      "-----sentiment-----\n",
      "Balanced accuracy 0.6984866665877545\n"
     ]
    }
   ],
   "source": [
    "# Ablations with the annotated data on balanced accuracy\n",
    "labs = [\"hate\", \"junksci\", \"prop\", \"sat\", \"click\", \"conspiracy\", \"sentiment\"]\n",
    "new_scores = [round((s-1)/5) for s in full_labels]\n",
    "full_features = full_vectors['ANNOT']\n",
    "sz = int(0.1*len(full_vecs))\n",
    "for i in range(1, 8):\n",
    "    print(f'-----{labs[i-1]}-----')\n",
    "    balacc = []\n",
    "    lmodel = sklearn.linear_model.LogisticRegression()\n",
    "    for j in range(10):\n",
    "        split_feat = full_features[0:j*sz]+full_features[(j+1)*sz:]\n",
    "        split_feat = [vec[:i-1] + vec[i:] for vec in split_feat]\n",
    "        lmodel.fit(split_feat, new_scores[0:j*sz]+new_scores[(j+1)*sz:])\n",
    "        test_feat = full_features[j*sz:(j+1)*sz]\n",
    "        test_feat = [vec[:i-1] + vec[i:] for vec in test_feat]\n",
    "        preds = lmodel.predict(test_feat)\n",
    "        balacc.append(metrics.balanced_accuracy_score(new_scores[j*sz:(j+1)*sz], preds))\n",
    "    print(\"Balanced accuracy\",np.mean(balacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d090674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----hate-----\n",
      "Balanced accuracy 0.5160988654334137\n",
      "-----junksci-----\n",
      "Balanced accuracy 0.5004219701456775\n",
      "-----prop-----\n",
      "Balanced accuracy 0.5301092056366887\n",
      "-----sat-----\n",
      "Balanced accuracy 0.5155529285804116\n",
      "-----click-----\n",
      "Balanced accuracy 0.49691330869737066\n",
      "-----conspiracy-----\n",
      "Balanced accuracy 0.5136507546673681\n",
      "-----sentiment-----\n",
      "Balanced accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "# Ablations with the FRESH output on balanced accuracy\n",
    "labs = [\"hate\", \"junksci\", \"prop\", \"sat\", \"click\", \"conspiracy\", \"sentiment\"]\n",
    "new_scores = [round((s-1)/5) for s in full_labels]\n",
    "full_features = full_vectors['FRESH']\n",
    "sz = int(0.1*len(full_vecs))\n",
    "for i in range(1, 8):\n",
    "    print(f'-----{labs[i-1]}-----')\n",
    "    balacc = []\n",
    "    lmodel = sklearn.linear_model.LogisticRegression()\n",
    "    for j in range(10):\n",
    "        split_feat = full_features[0:j*sz]+full_features[(j+1)*sz:]\n",
    "        split_feat = [vec[:i-1] + vec[i:] for vec in split_feat]\n",
    "        lmodel.fit(split_feat, new_scores[0:j*sz]+new_scores[(j+1)*sz:])\n",
    "        test_feat = full_features[j*sz:(j+1)*sz]\n",
    "        test_feat = [vec[:i-1] + vec[i:] for vec in test_feat]\n",
    "        preds = lmodel.predict(test_feat)\n",
    "        balacc.append(metrics.balanced_accuracy_score(new_scores[j*sz:(j+1)*sz], preds))\n",
    "    print(\"Balanced accuracy\",np.mean(balacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2679a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----hate-----\n",
      "Balanced accuracy 0.5213166147696902\n",
      "-----junksci-----\n",
      "Balanced accuracy 0.5204200686035501\n",
      "-----prop-----\n",
      "Balanced accuracy 0.5106608955086017\n",
      "-----sat-----\n",
      "Balanced accuracy 0.5204200686035501\n",
      "-----click-----\n",
      "Balanced accuracy 0.5126592296978083\n",
      "-----conspiracy-----\n",
      "Balanced accuracy 0.514725902437785\n",
      "-----sentiment-----\n",
      "Balanced accuracy 0.49206724727671247\n"
     ]
    }
   ],
   "source": [
    "# Ablations with the BERT output on balanced accuracy\n",
    "labs = [\"hate\", \"junksci\", \"prop\", \"sat\", \"click\", \"conspiracy\", \"sentiment\"]\n",
    "new_scores = [round((s-1)/5) for s in full_labels]\n",
    "full_features = full_vectors['BERT']\n",
    "sz = int(0.1*len(full_vecs))\n",
    "for i in range(1, 8):\n",
    "    print(f'-----{labs[i-1]}-----')\n",
    "    balacc = []\n",
    "    lmodel = sklearn.linear_model.LogisticRegression()\n",
    "    for j in range(10):\n",
    "        split_feat = full_features[0:j*sz]+full_features[(j+1)*sz:]\n",
    "        split_feat = [vec[:i-1] + vec[i:] for vec in split_feat]\n",
    "        lmodel.fit(split_feat, new_scores[0:j*sz]+new_scores[(j+1)*sz:])\n",
    "        test_feat = full_features[j*sz:(j+1)*sz]\n",
    "        test_feat = [vec[:i-1] + vec[i:] for vec in test_feat]\n",
    "        preds = lmodel.predict(test_feat)\n",
    "        balacc.append(metrics.balanced_accuracy_score(new_scores[j*sz:(j+1)*sz], preds))\n",
    "    print(\"Balanced accuracy\",np.mean(balacc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
